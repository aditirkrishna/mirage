{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIRAGE++ Notebook 1: Theory & Motivation\n",
    "\n",
    "## Why Do We Need a New Linear Regression Model in Quant Finance?\n",
    "\n",
    "Linear regression is the backbone of quant finance, but it has limitations: overfitting, instability, and poor generalization in noisy, high-dimensional data. This notebook explains why entropy regularization and mirror descent offer a new, robust approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ordinary Least Squares (OLS)\n",
    "\n",
    "OLS finds weights $\\theta$ that minimize the squared error between predictions and actual values.\n",
    "\n",
    "$L(\\theta) = \\|X\\theta - y\\|^2$\n",
    "\n",
    "But OLS can overfit, especially with many features or noisy data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(100, 5)\n",
    "true_theta = np.array([0.2, 0.1, 0.4, 0.1, 0.2])\n",
    "y = X @ true_theta + np.random.randn(100) * 0.1\n",
    "\n",
    "ols = LinearRegression()\n",
    "ols.fit(X, y)\n",
    "print('OLS weights:', ols.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ridge and Lasso Regularization\n",
    "\n",
    "Ridge (L2) and Lasso (L1) add penalties to the loss to prevent overfitting.\n",
    "\n",
    "$L_{ridge}(\\theta) = \\|X\\theta - y\\|^2 + \\alpha \\|\\theta\\|^2$\n",
    "$L_{lasso}(\\theta) = \\|X\\theta - y\\|^2 + \\alpha \\|\\theta\\|_1$\n",
    "\n",
    "But these don't encourage diversity or interpretability in weights."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X, y)\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "print('Ridge weights:', ridge.coef_)\n",
    "print('Lasso weights:', lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entropy Regularization: A New Approach\n",
    "\n",
    "Entropy regularization encourages weights to be spread out (diversified), not just small.\n",
    "\n",
    "$H(\\theta) = -\\sum_i \\theta_i \\log \\theta_i$\n",
    "\n",
    "This is especially useful in finance, where we want diversified portfolios or signal blends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mirror Descent: Smarter Optimization\n",
    "\n",
    "Mirror descent generalizes gradient descent by using a different geometry (Bregman divergence, e.g., KL).\n",
    "\n",
    "This is ideal for weights that must be positive and sum to 1 (like probabilities or portfolio weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next: Build the MIRAGE++ Model\n",
    "\n",
    "In the next notebook, we'll implement the entropy-regularized linear regression with mirror descent, and see how it compares to OLS, Ridge, and Lasso."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
