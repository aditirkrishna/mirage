{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIRAGE++ Notebook 4: Application to Real Financial Data\n",
    "\n",
    "In this notebook, we'll use MIRAGE++ on real financial data. We'll download stock prices, engineer features, fit the model, and compare it to OLS, Ridge, and Lasso. Every step is explained for clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Downloading Real Financial Data\n",
    "\n",
    "We'll use the `yfinance` package to get daily stock prices for several assets. If you don't have it, install with `!pip install yfinance`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "tickers = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'META']\n",
    "data = yf.download(tickers, start='2020-01-01', end='2023-01-01')['Adj Close']\n",
    "data = data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering: Lagged Returns and Technical Indicators\n",
    "\n",
    "We'll create features like lagged returns and moving averages for each stock."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "returns = data.pct_change().dropna()\n",
    "features = []\n",
    "for lag in range(1, 6):\n",
    "    features.append(returns.shift(lag))\n",
    "ma = data.rolling(5).mean().pct_change().dropna()\n",
    "features.append(ma)\n",
    "X = pd.concat(features, axis=1).dropna()\n",
    "X = X.loc[X.index.intersection(returns.index)]\n",
    "y = returns.mean(axis=1).loc[X.index]  # Predict average return across assets\n",
    "print('Feature matrix shape:', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Standardize Features\n",
    "\n",
    "It's good practice to standardize features for regression."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y = y.values\n",
    "print('Standardized feature matrix shape:', X_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fit MIRAGE++ to the Real Data\n",
    "\n",
    "We'll use the same MIRAGE++ code as before."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def entropy(theta, epsilon=1e-8):\n",
    "    theta = np.clip(theta, epsilon, 1.0)\n",
    "    return -np.sum(theta * np.log(theta))\n",
    "\n",
    "def loss(X, y, theta, lam):\n",
    "    preds = X @ theta\n",
    "    mse = np.mean((preds - y)**2)\n",
    "    ent = entropy(theta)\n",
    "    return mse + lam * ent\n",
    "\n",
    "def gradient(X, y, theta, lam):\n",
    "    preds = X @ theta\n",
    "    grad_mse = 2 * X.T @ (preds - y) / len(y)\n",
    "    grad_entropy = -1 - np.log(np.clip(theta, 1e-8, 1.0))\n",
    "    return grad_mse + lam * grad_entropy\n",
    "\n",
    "def mirror_descent_step(grad, theta_t, eta):\n",
    "    theta_new = theta_t * np.exp(-eta * grad)\n",
    "    theta_new = np.clip(theta_new, 1e-12, None)\n",
    "    return theta_new / np.sum(theta_new)\n",
    "\n",
    "def fit_mirage(X, y, lam=0.1, eta=0.2, n_iters=300):\n",
    "    n = X.shape[1]\n",
    "    theta = np.ones(n) / n\n",
    "    loss_hist = []\n",
    "    for i in range(n_iters):\n",
    "        grad = gradient(X, y, theta, lam)\n",
    "        theta = mirror_descent_step(grad, theta, eta)\n",
    "        loss_hist.append(loss(X, y, theta, lam))\n",
    "    return theta, loss_hist\n",
    "\n",
    "theta_mirage, loss_hist = fit_mirage(X_scaled, y, lam=0.1, eta=0.2, n_iters=300)\n",
    "print('MIRAGE++ weights:', theta_mirage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare to OLS, Ridge, and Lasso\n",
    "\n",
    "Let's fit standard models and compare their weights and prediction errors."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "ols = LinearRegression().fit(X_scaled, y)\n",
    "ridge = Ridge(alpha=0.1).fit(X_scaled, y)\n",
    "lasso = Lasso(alpha=0.1).fit(X_scaled, y)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.bar(np.arange(X_scaled.shape[1])-0.3, ols.coef_, width=0.2, label='OLS')\n",
    "plt.bar(np.arange(X_scaled.shape[1])-0.1, ridge.coef_, width=0.2, label='Ridge')\n",
    "plt.bar(np.arange(X_scaled.shape[1])+0.1, lasso.coef_, width=0.2, label='Lasso')\n",
    "plt.bar(np.arange(X_scaled.shape[1])+0.3, theta_mirage, width=0.2, label='MIRAGE++')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Weight')\n",
    "plt.title('Model Weights Comparison (Real Data)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prediction Error Comparison\n",
    "\n",
    "Let's compare mean squared error (MSE) for each model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "y_pred_ols = ols.predict(X_scaled)\n",
    "y_pred_ridge = ridge.predict(X_scaled)\n",
    "y_pred_lasso = lasso.predict(X_scaled)\n",
    "y_pred_mirage = X_scaled @ theta_mirage\n",
    "\n",
    "print('OLS MSE:', mse(y, y_pred_ols))\n",
    "print('Ridge MSE:', mse(y, y_pred_ridge))\n",
    "print('Lasso MSE:', mse(y, y_pred_lasso))\n",
    "print('MIRAGE++ MSE:', mse(y, y_pred_mirage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sharpe Ratio Comparison\n",
    "\n",
    "The Sharpe ratio measures risk-adjusted return. Let's compare it for each model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sharpe_ratio(returns):\n",
    "    return np.mean(returns) / np.std(returns)\n",
    "\n",
    "print('OLS Sharpe:', sharpe_ratio(y_pred_ols))\n",
    "print('Ridge Sharpe:', sharpe_ratio(y_pred_ridge))\n",
    "print('Lasso Sharpe:', sharpe_ratio(y_pred_lasso))\n",
    "print('MIRAGE++ Sharpe:', sharpe_ratio(y_pred_mirage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interpretability and Diversity\n",
    "\n",
    "MIRAGE++ weights are positive and sum to 1, making them easy to interpret as allocations or probabilities.\n",
    "\n",
    "Let's check this property."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('MIRAGE++ weights (should be positive):', theta_mirage)\n",
    "print('Sum of MIRAGE++ weights:', np.sum(theta_mirage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "- MIRAGE++ works on real financial data and produces interpretable, diversified weights.\n",
    "- It performs competitively with OLS, Ridge, and Lasso.\n",
    "- Weights are always positive and sum to 1.\n",
    "\n",
    "In the next notebook, we'll explore advanced geometry and extensions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
